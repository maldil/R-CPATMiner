<html><h3>3a4338bc06c9f207f8911dd16c5084759c4ae990,stanza/utils/prepare_tokenizer_treebank.py,,prepare_ud_dataset,#,72
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>

    shutil.copyfile(input_conllu,
                    f"{tokenizer_dir}/{short_name}.{dataset}.gold.conllu")
    <a id="change">shutil.copyfile(input_txt,
                    f"{tokenizer_dir}/{short_name}.{dataset}.txt")</a>

    if short_language == "vi":
        postprocess_vietnamese_tokenizer_data.main([input_txt,
                                                    "--char_level_pred", f"{tokenizer_dir}/{short_name}-ud-{dataset}.toklabels",</code></pre><h3>After Change</h3><pre><code class='java'>
    input_conllu = find_treebank_dataset_file(treebank, udbase_dir, dataset, "conllu")
    input_conllu_copy = f"{tokenizer_dir}/{short_name}.{dataset}.gold.conllu"

    <a id="change">if short_name == "sl_ssj":
        preprocess_ssj_data.process(input_txt, input_conllu, input_txt_copy, input_conllu_copy)
    else:
        os.makedirs(tokenizer_dir, exist_ok=True)
        shutil.copyfile(input_txt, input_txt_copy)
        shutil.copyfile(input_conllu, input_conllu_copy)

   </a> prepare_tokenizer_data.main([input_txt_copy,
                                 input_conllu_copy,
                                 "-o", f"{tokenizer_dir}/{short_name}-ud-{dataset}.toklabels",
                                 "-m", f"{tokenizer_dir}/{short_name}-ud-{dataset}-mwt.json"])</code></pre><img src="3845202.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/stanfordnlp/stanza/commit/3a4338bc06c9f207f8911dd16c5084759c4ae990#diff-0830f7d7e8814bbfd6885da283c22709c0a397b0f921602cff4f1ad4ff698759L74' target='_blank'>Link</a></div><div id='project'> Project Name: stanfordnlp/stanza</div><div id='commit'> Commit Name: 3a4338bc06c9f207f8911dd16c5084759c4ae990</div><div id='time'> Time: 2020-11-21</div><div id='author'> Author: horatio@gmail.com</div><div id='file'> File Name: stanza/utils/prepare_tokenizer_treebank.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: prepare_ud_dataset</div><BR><BR><div id='link'><a href='https://github.com/stanfordnlp/stanza/commit/3a4338bc06c9f207f8911dd16c5084759c4ae990#diff-0830f7d7e8814bbfd6885da283c22709c0a397b0f921602cff4f1ad4ff698759L74' target='_blank'>Link</a></div><div id='project'> Project Name: stanfordnlp/stanza</div><div id='commit'> Commit Name: 3a4338bc06c9f207f8911dd16c5084759c4ae990</div><div id='time'> Time: 2020-11-21</div><div id='author'> Author: horatio@gmail.com</div><div id='file'> File Name: stanza/utils/prepare_tokenizer_treebank.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: prepare_ud_dataset</div><BR><BR><div id='link'><a href='https://github.com/stanfordnlp/stanza/commit/5df5c95a05bdca1ae618545e65d1a1ae6ef5f13f#diff-0830f7d7e8814bbfd6885da283c22709c0a397b0f921602cff4f1ad4ff698759L157' target='_blank'>Link</a></div><div id='project'> Project Name: stanfordnlp/stanza</div><div id='commit'> Commit Name: 5df5c95a05bdca1ae618545e65d1a1ae6ef5f13f</div><div id='time'> Time: 2020-11-21</div><div id='author'> Author: horatio@gmail.com</div><div id='file'> File Name: stanza/utils/prepare_tokenizer_treebank.py</div><div id='class'> Class Name: </div><div id='method'> Method Name: prepare_ud_dataset</div><BR>