<link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
            return [i-1, b, outputs]

        cond = lambda i, b, inputs_hat: i &gt; 0
        loop_vars = <a id="change">[K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]</a>
        <a id="change">_, self.bias, outputs = tf.while_loop(cond, body, loop_vars)</a>

        
        &#47&#47 Routing algorithm V2. Seems not right. This may duplicate tensors by self.num_routing times.
        for _ in range(self.num_routing):</code></pre><h3>After Change</h3><pre><code class='java'>
            c = K.softmax(self.bias)
            c_expand = K.expand_dims(K.expand_dims(K.expand_dims(c, 2), 2), 0)
            outputs = K.sum(c_expand * inputs_hat, 1, keepdims=True)
            <a id="change">outputs = squash(outputs)</a>
            self.bias = K.update(self.bias, self.bias + K.sum(inputs_hat * outputs, [0, -2, -1]))

        &#47&#47 Handling with no routing scenario. Prior bias will always be zero.
        if self.num_routing == 0:
            <a id="change">c = K.softmax(self.bias)</a>
            <a id="change">c_expand = K.expand_dims(K.expand_dims(K.expand_dims(c, 2), 2), 0)</a>
            <a id="change">outputs = squash(K.sum(c_expand * inputs_hat, 1, keepdims=True))</a>

        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])

    def compute_output_shape(self, input_shape):</code></pre>