<html><h3>ee9cadefd8d3b96469f97453db73a117fee5a009,mmf/models/vilbert.py,BertBiAttention,forward,#BertBiAttention#,330
</h3><link rel="stylesheet" href="../../../../default.css">
<script src="../../../../highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><h3></h3><h3>Before Change</h3><pre><code class='java'>
        &#47&#47 attention_scores2 = attention_scores2 + co_attention_mask

        &#47&#47 Normalize the attention scores to probabilities.
        attention_probs2 = <a id="change">nn.Softmax(dim=-1)(attention_scores2)</a>

        &#47&#47 This is actually dropping out entire tokens to attend to, which might
        &#47&#47 seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs2 = self.dropout2(attention_probs2)</code></pre><h3>After Change</h3><pre><code class='java'>
        &#47&#47 attention_scores2 = attention_scores2 + co_attention_mask

        &#47&#47 Normalize the attention scores to probabilities.
        attention_probs2 = <a id="change">nn.functional.softmax(attention_scores2, dim=-1)</a>

        &#47&#47 This is actually dropping out entire tokens to attend to, which might
        &#47&#47 seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs2 = self.dropout2(attention_probs2)</code></pre><img src="36797701.png" alt="Italian Trulli"   style="width:500px;height:500px;"><div id='inPattern'>In pattern: SUPERPATTERN</div><BR><div id='frequency'>Frequency: 3</div><BR><div id='size'>Non-data size: 4</div><BR><h3>Instances</h3><BR><div id='link'><a href='https://github.com/facebookresearch/pythia/commit/ee9cadefd8d3b96469f97453db73a117fee5a009#diff-e0be7eaa9b3fa2f7d41b51cda09993a6207c948c428e2312dfce8679fc7c4812L366' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/pythia</div><div id='commit'> Commit Name: ee9cadefd8d3b96469f97453db73a117fee5a009</div><div id='time'> Time: 2020-10-14</div><div id='author'> Author: mengq@fb.com</div><div id='file'> File Name: mmf/models/vilbert.py</div><div id='class'> Class Name: BertBiAttention</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/pythia/commit/ee9cadefd8d3b96469f97453db73a117fee5a009#diff-e0be7eaa9b3fa2f7d41b51cda09993a6207c948c428e2312dfce8679fc7c4812L74' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/pythia</div><div id='commit'> Commit Name: ee9cadefd8d3b96469f97453db73a117fee5a009</div><div id='time'> Time: 2020-10-14</div><div id='author'> Author: mengq@fb.com</div><div id='file'> File Name: mmf/models/vilbert.py</div><div id='class'> Class Name: BertSelfAttention</div><div id='method'> Method Name: forward</div><BR><BR><div id='link'><a href='https://github.com/facebookresearch/pythia/commit/ee9cadefd8d3b96469f97453db73a117fee5a009#diff-e0be7eaa9b3fa2f7d41b51cda09993a6207c948c428e2312dfce8679fc7c4812L169' target='_blank'>Link</a></div><div id='project'> Project Name: facebookresearch/pythia</div><div id='commit'> Commit Name: ee9cadefd8d3b96469f97453db73a117fee5a009</div><div id='time'> Time: 2020-10-14</div><div id='author'> Author: mengq@fb.com</div><div id='file'> File Name: mmf/models/vilbert.py</div><div id='class'> Class Name: BertImageSelfAttention</div><div id='method'> Method Name: forward</div><BR>